{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "from torchtext import data, datasets\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    tokens = []\n",
    "    for word in nltk.word_tokenize(text):\n",
    "        word = word.lower()\n",
    "        if not word.isnumeric():\n",
    "            tokens.append(word)\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = data.Field(sequential=False, use_vocab=False)\n",
    "TEXT = data.Field(sequential=True, tokenize=tokenizer, include_lengths=True, batch_first=True, fix_length=144, eos_token='eos')\n",
    "LABEL = data.Field(sequential=False, use_vocab=False)\n",
    "\n",
    "train, test_set = data.TabularDataset.splits(path='./', train='train.csv', test='test.csv',\n",
    "                                        format='csv', fields=[('id', ID), ('text', TEXT), ('label', LABEL)],\n",
    "                                         skip_header=True, csv_reader_params={\"delimiter\":','})\n",
    "TEXT.build_vocab(train, vectors=\"glove.840B.300d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchtext will initialize <pad> <eos> and <unk> vectors to zeros\n",
    "# use different random vectors to differentiate them\n",
    "origin_vocab_vec = TEXT.vocab\n",
    "for i in range(3):\n",
    "    origin_vocab_vec.vectors[i] = torch.rand(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def get_doc(file_path, stoi, keep_sents=20, sent_length=40):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        tr = csv.reader(f, delimiter=',')\n",
    "        docs = []\n",
    "        sents_len = []\n",
    "        docs_label = []\n",
    "        docs_len = [] \n",
    "        ids = []\n",
    "        for i, row in enumerate(tr):\n",
    "            if i == 0: continue\n",
    "            doc = []\n",
    "            sent_len = []\n",
    "            docs_label.append(int(row[2]))\n",
    "            sents = row[1].split('.')\n",
    "            for j, s in enumerate(sents):\n",
    "                if j >= keep_sents: break\n",
    "                s = s.split()\n",
    "                if len(s) == 0:\n",
    "                    continue\n",
    "                sent = []\n",
    "                for k, w in enumerate(s):\n",
    "                    if k >= sent_length: break\n",
    "                    sent.append(stoi.get(w.lower(), stoi['<pad>']))\n",
    "                sent_len.append(len(sent))\n",
    "                while len(sent) < sent_length: sent.append(stoi['<pad>'])\n",
    "                doc.append(sent)\n",
    "            docs_len.append(len(doc))\n",
    "            while len(doc) < keep_sents:\n",
    "                doc.append([stoi['<pad>'] for _ in range(sent_length)])\n",
    "                sent_len.append(1)\n",
    "            docs.append(doc)\n",
    "            sents_len.append(sent_len)\n",
    "            ids.append(int(row[0]))\n",
    "    res = [docs, docs_label, sents_len, docs_len, ids]\n",
    "    for i in range(len(res)):\n",
    "        res[i] = torch.cuda.LongTensor(res[i])\n",
    "    return tuple(res)\n",
    "train, train_y, train_s_len, train_d_len, train_id = get_doc(\"train.csv\", TEXT.vocab.stoi, 30, 30)\n",
    "test, test_y, test_s_len, test_d_len, test_id = get_doc(\"test.csv\", TEXT.vocab.stoi, 30, 30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class torchIter:\n",
    "    def __init__(self, batchsize, doc, doc_labels, doc_len, sent_len, ids):\n",
    "        self.samples = doc.shape[0]\n",
    "        self.batchsize = batchsize\n",
    "        self.ctimes = (self.samples + batchsize - 1) // batchsize\n",
    "        self.params = (doc, doc_labels, doc_len, sent_len, ids)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        self.ctr = 0\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.ctr >= self.ctimes:\n",
    "            raise StopIteration\n",
    "        st = self.batchsize * self.ctr\n",
    "        ed = st + self.batchsize\n",
    "        self.ctr += 1\n",
    "        return tuple(p[st:ed] for p in self.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSortedOrder(lens):\n",
    "    sortedLen, fwdOrder = torch.sort(\n",
    "        lens.contiguous().view(-1), dim=0, descending=True)\n",
    "    _, bwdOrder = torch.sort(fwdOrder)\n",
    "    sortedLen = sortedLen.cpu().numpy().tolist()\n",
    "    return sortedLen, fwdOrder, bwdOrder\n",
    "\n",
    "def dynamicRNN(rnnModel,\n",
    "               seqInput,\n",
    "               seqLens):\n",
    "    '''\n",
    "    Inputs:\n",
    "        rnnModel     : Any torch.nn RNN model\n",
    "        seqInput     : (batchSize, maxSequenceLength, embedSize)\n",
    "                        Input sequence tensor (padded) for RNN model\n",
    "        seqLens      : batchSize length torch.LongTensor or numpy array\n",
    "        initialState : Initial (hidden, cell) states of RNN\n",
    "\n",
    "    Output:\n",
    "        A single tensor of shape (batchSize, rnnHiddenSize) corresponding\n",
    "        to the outputs of the RNN model at the last time step of each input\n",
    "        sequence. If returnStates is True, also return a tuple of hidden\n",
    "        and cell states at every layer of size (num_layers, batchSize,\n",
    "        rnnHiddenSize)\n",
    "    '''\n",
    "    sortedLen, fwdOrder, bwdOrder = getSortedOrder(seqLens)\n",
    "    sortedSeqInput = seqInput.index_select(dim=0, index=fwdOrder)\n",
    "    packedSeqInput = nn.utils.rnn.pack_padded_sequence(\n",
    "        sortedSeqInput, lengths=sortedLen, batch_first=True)\n",
    "\n",
    "    output, _ = rnnModel(packedSeqInput)\n",
    "    output, _ = nn.utils.rnn.pad_packed_sequence(output, batch_first=True)\n",
    "\n",
    "    rnn_output = output.index_select(dim=0, index=bwdOrder)\n",
    "    return rnn_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, vocab, dropout, hidden_size, layer1, device=torch.device('cpu')):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding_size = 300\n",
    "        self.embedding = nn.Embedding.from_pretrained(vocab.vectors, freeze=True)\n",
    "        self.rnn = nn.GRU(input_size=self.embedding_size, hidden_size=self.hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.drnn = nn.GRU(input_size=self.hidden_size * 2, hidden_size=self.hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.h_u = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.h_u2 = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.u_w = nn.Parameter(nn.init.xavier_normal_(torch.Tensor(1, 1, self.hidden_size)))\n",
    "        self.u_d = nn.Parameter(nn.init.xavier_normal_(torch.Tensor(1, 1, self.hidden_size)))\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.softmax = nn.Softmax(-1)\n",
    "        self.m_hid = nn.Linear(self.hidden_size * 2, layer1)\n",
    "        self.score = nn.Linear(layer1, 5)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "    def forward(self, data, doc_len, sent_len):\n",
    "        bs, dc, st = data.shape\n",
    "        data = self.embedding(data)\n",
    "        data = self.dropout(data)\n",
    "        data = data.view(bs*dc, st, -1)\n",
    "        data_len = sent_len.view(bs*dc)\n",
    "        output = dynamicRNN(self.rnn, data, data_len)\n",
    "        u = self.tanh(self.h_u(output))\n",
    "        alpha = self.softmax(torch.sum(self.u_w * u, -1))\n",
    "        s = torch.sum(alpha.unsqueeze(-1) * output, 1)\n",
    "        s = s.view(bs, dc, -1)\n",
    "        output = dynamicRNN(self.drnn, s, doc_len)\n",
    "        u2 = self.tanh(self.h_u2(output))\n",
    "        alpha2 = self.softmax(torch.sum(self.u_d * u2, -1))\n",
    "        d = torch.sum(alpha2.unsqueeze(-1) * output, 1)\n",
    "        attn_hid = self.relu(self.m_hid(d))\n",
    "        attn_hid = self.dropout(attn_hid)\n",
    "        score = self.score(attn_hid)\n",
    "        return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters setting and initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "BATCHSIZE = 256\n",
    "torch.cuda.empty_cache()\n",
    "vocab = copy.deepcopy(TEXT.vocab)\n",
    "m = nn.DataParallel(Model(vocab, 0.1, 100, 100, device=DEVICE).to(DEVICE))\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(m.parameters(), 1e-3, weight_decay=1e-4)\n",
    "train_iter = torchIter(BATCHSIZE, train, train_y, train_d_len, train_s_len, train_id)\n",
    "test_iter = torchIter(BATCHSIZE, test, test_y, test_d_len, test_s_len, test_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5-fold validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "newk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/data/dy_fyp/miniconda3/envs/fyp/lib/python3.7/site-packages/ipykernel_launcher.py:31: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/home/data/dy_fyp/miniconda3/envs/fyp/lib/python3.7/site-packages/ipykernel_launcher.py:34: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 1.3718334 train acc: 0.43429688 val acc: 0.3921274\n",
      "train loss: 1.1178372 train acc: 0.4935156 val acc: 0.53155047\n",
      "train loss: 0.91026473 train acc: 0.58382815 val acc: 0.60847354\n",
      "train loss: 0.83833504 train acc: 0.6230469 val acc: 0.6207933\n",
      "train loss: 0.8054542 train acc: 0.64085937 val acc: 0.6340144\n",
      "train loss: 0.77645576 train acc: 0.65335935 val acc: 0.6451322\n",
      "train loss: 0.7526119 train acc: 0.6671875 val acc: 0.65084136\n",
      "train loss: 0.73245454 train acc: 0.67296875 val acc: 0.6550481\n",
      "train loss: 0.7156479 train acc: 0.68460935 val acc: 0.6583534\n",
      "train loss: 0.7016935 train acc: 0.6926563 val acc: 0.66376203\n",
      "train loss: 0.6865207 train acc: 0.69742185 val acc: 0.65865386\n",
      "train loss: 0.669266 train acc: 0.7084375 val acc: 0.6547476\n",
      "train loss: 0.64975494 train acc: 0.7153906 val acc: 0.6577524\n",
      "train loss: 0.62654 train acc: 0.7272656 val acc: 0.6547476\n",
      "train loss: 0.61174333 train acc: 0.7335156 val acc: 0.64873797\n",
      "newk\n",
      "train loss: 1.4760774 train acc: 0.40476564 val acc: 0.4329928\n",
      "train loss: 1.2691917 train acc: 0.43195313 val acc: 0.4591346\n",
      "train loss: 1.0499985 train acc: 0.51984376 val acc: 0.57572114\n",
      "train loss: 0.92075074 train acc: 0.5821875 val acc: 0.61688703\n",
      "train loss: 0.8593134 train acc: 0.61148435 val acc: 0.6234976\n",
      "train loss: 0.8331135 train acc: 0.63054687 val acc: 0.64813703\n",
      "train loss: 0.79465884 train acc: 0.64796877 val acc: 0.6532452\n",
      "train loss: 0.77064055 train acc: 0.6602344 val acc: 0.6550481\n",
      "train loss: 0.7491477 train acc: 0.6675 val acc: 0.6595553\n",
      "train loss: 0.7319116 train acc: 0.6773437 val acc: 0.66165864\n",
      "train loss: 0.71672785 train acc: 0.6830469 val acc: 0.6613582\n",
      "train loss: 0.7012261 train acc: 0.6892969 val acc: 0.6613582\n",
      "train loss: 0.682699 train acc: 0.70023435 val acc: 0.65024036\n",
      "train loss: 0.6620629 train acc: 0.7099219 val acc: 0.65084136\n",
      "train loss: 0.63957393 train acc: 0.7224219 val acc: 0.6514423\n",
      "newk\n",
      "train loss: 1.4771663 train acc: 0.41484374 val acc: 0.43239182\n",
      "train loss: 1.2914509 train acc: 0.4310156 val acc: 0.43840143\n",
      "train loss: 1.0652864 train acc: 0.5119531 val acc: 0.5640024\n",
      "train loss: 0.92191863 train acc: 0.5772656 val acc: 0.60336536\n",
      "train loss: 0.849683 train acc: 0.6203125 val acc: 0.62740386\n",
      "train loss: 0.8161762 train acc: 0.63539064 val acc: 0.6340144\n",
      "train loss: 0.7831279 train acc: 0.65421873 val acc: 0.6349159\n",
      "train loss: 0.76395243 train acc: 0.6627344 val acc: 0.6451322\n",
      "train loss: 0.7374894 train acc: 0.6755469 val acc: 0.6484375\n",
      "train loss: 0.7169375 train acc: 0.6839844 val acc: 0.6511418\n",
      "train loss: 0.702468 train acc: 0.6908594 val acc: 0.6541466\n",
      "train loss: 0.68151367 train acc: 0.70335937 val acc: 0.65685093\n",
      "train loss: 0.6672335 train acc: 0.7066406 val acc: 0.6499399\n",
      "train loss: 0.65262324 train acc: 0.7140625 val acc: 0.64453125\n",
      "train loss: 0.6284319 train acc: 0.7282031 val acc: 0.6436298\n",
      "newk\n",
      "train loss: 1.4592711 train acc: 0.42140624 val acc: 0.40985578\n",
      "train loss: 1.2736534 train acc: 0.43609375 val acc: 0.40985578\n",
      "train loss: 1.1237694 train acc: 0.48773438 val acc: 0.52584136\n",
      "train loss: 0.9372117 train acc: 0.5789844 val acc: 0.5892428\n",
      "train loss: 0.8668785 train acc: 0.6109375 val acc: 0.61628604\n",
      "train loss: 0.8242786 train acc: 0.63320315 val acc: 0.6277043\n",
      "train loss: 0.79436874 train acc: 0.645 val acc: 0.6301082\n",
      "train loss: 0.7809388 train acc: 0.6534375 val acc: 0.64032453\n",
      "train loss: 0.7761409 train acc: 0.6515625 val acc: 0.6298077\n",
      "train loss: 0.78227115 train acc: 0.651875 val acc: 0.6201923\n",
      "train loss: 0.73433626 train acc: 0.6761719 val acc: 0.6379207\n",
      "train loss: 0.7245565 train acc: 0.6761719 val acc: 0.6361178\n",
      "train loss: 0.7031259 train acc: 0.686875 val acc: 0.6418269\n",
      "train loss: 0.68316126 train acc: 0.69554687 val acc: 0.64092547\n",
      "train loss: 0.6697573 train acc: 0.7046875 val acc: 0.6373197\n",
      "newk\n",
      "train loss: 1.5162585 train acc: 0.38889724 val acc: 0.49609375\n",
      "train loss: 1.3354741 train acc: 0.4200721 val acc: 0.503196\n",
      "train loss: 1.0770713 train acc: 0.5090895 val acc: 0.5802557\n",
      "train loss: 0.93315434 train acc: 0.56933594 val acc: 0.6360085\n",
      "train loss: 0.8729433 train acc: 0.6048678 val acc: 0.637429\n",
      "train loss: 0.8346751 train acc: 0.62252104 val acc: 0.6605114\n",
      "train loss: 0.8116883 train acc: 0.6361178 val acc: 0.67045456\n",
      "train loss: 0.78309494 train acc: 0.6519681 val acc: 0.6740057\n",
      "train loss: 0.7599176 train acc: 0.66008115 val acc: 0.6796875\n",
      "train loss: 0.740867 train acc: 0.67232573 val acc: 0.6796875\n",
      "train loss: 0.72749543 train acc: 0.67442906 val acc: 0.6764915\n",
      "train loss: 0.71661884 train acc: 0.67938703 val acc: 0.6754261\n",
      "train loss: 0.6970972 train acc: 0.69140625 val acc: 0.68039775\n",
      "train loss: 0.67568576 train acc: 0.70342547 val acc: 0.67578125\n",
      "train loss: 0.6588222 train acc: 0.7123648 val acc: 0.67009944\n",
      "avg val acc: [0.43067956 0.4673239  0.5705605  0.6125372  0.62741816 0.64254713\n",
      " 0.6471974  0.6532738  0.65438986 0.6545139  0.6571181  0.656312\n",
      " 0.65525794 0.65265375 0.6496156 ]\n"
     ]
    }
   ],
   "source": [
    "k_fold = 5\n",
    "fold_len = math.ceil(math.ceil(len(train) / BATCHSIZE) / k_fold)\n",
    "print(fold_len)\n",
    "epochs = 15\n",
    "avg_acc = [[] for _ in range(epochs)]\n",
    "start_dict = copy.deepcopy(m.state_dict())\n",
    "nn.utils.clip_grad_norm_(m.parameters(), 10)\n",
    "for k in range(k_fold):\n",
    "    print(\"newk\")\n",
    "    m.load_state_dict(start_dict)\n",
    "    for e in range(epochs):\n",
    "        avg = []\n",
    "        val_set = []\n",
    "        val_loss = []\n",
    "        train_acc = []\n",
    "        m.train()\n",
    "        for i, batch in enumerate(train_iter):\n",
    "            if i >= k * fold_len and i < (k + 1) * fold_len:\n",
    "                continue\n",
    "            y = batch[1] - 1 # for torch\n",
    "            score = m(batch[0], batch[2], batch[3])\n",
    "            pred = torch.argmax(score, -1)\n",
    "            train_acc.append((torch.sum(pred == y).float() / y.shape[0]).data)\n",
    "            l = loss(score, y)\n",
    "            avg.append(l.data)\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "        m.eval()\n",
    "        for i, batch in enumerate(train_iter):\n",
    "            if not (i >= k * fold_len and i < (k + 1) * fold_len):\n",
    "                continue\n",
    "            y = batch[1]\n",
    "            score = nn.functional.softmax(m(batch[0], batch[2], batch[3]))\n",
    "            pred = torch.argmax(score, -1) + 1\n",
    "            l = torch.sum(pred == y).float() / y.shape[0]\n",
    "            val_loss.append(l.data)\n",
    "            avg_acc[e].append(l.data)\n",
    "        print('train loss:', np.mean(avg), 'train acc:', np.mean(train_acc), 'val acc:', np.mean(val_loss))\n",
    "print('avg val acc:', np.mean(avg_acc, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Based on validation result, choose the best epoch, retrain the model on training set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/data/dy_fyp/miniconda3/envs/fyp/lib/python3.7/site-packages/ipykernel_launcher.py:31: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.2997, device='cuda:0')\n",
      "tensor(1.0676, device='cuda:0')\n",
      "tensor(0.8935, device='cuda:0')\n",
      "tensor(0.8238, device='cuda:0')\n",
      "tensor(0.7700, device='cuda:0')\n",
      "tensor(0.7152, device='cuda:0')\n",
      "tensor(0.6988, device='cuda:0')\n",
      "tensor(0.7019, device='cuda:0')\n",
      "tensor(0.6860, device='cuda:0')\n",
      "tensor(0.6393, device='cuda:0')\n",
      "tensor(0.6233, device='cuda:0')\n",
      "tensor(0.5847, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "m.load_state_dict(start_dict)\n",
    "m.train()\n",
    "for e in range(12):\n",
    "    for batch in train_iter:\n",
    "        y = batch[1] - 1 # for torch\n",
    "        score = m(batch[0], batch[2], batch[3])\n",
    "        l = loss(score, y)\n",
    "        optimizer.zero_grad()\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "    print(l.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate the result on testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/data/dy_fyp/miniconda3/envs/fyp/lib/python3.7/site-packages/ipykernel_launcher.py:31: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "/home/data/dy_fyp/miniconda3/envs/fyp/lib/python3.7/site-packages/ipykernel_launcher.py:6: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "m.eval()\n",
    "id = []\n",
    "y = []\n",
    "for batch in test_iter:\n",
    "    score = m(batch[0], batch[2], batch[3])\n",
    "    c = nn.functional.softmax(score)\n",
    "    pred = torch.argmax(c, -1) + 1\n",
    "    id.append(batch[4])\n",
    "    y.append(pred)\n",
    "id = torch.cat(id, 0)\n",
    "y = torch.cat(y, 0)\n",
    "sub_df = pd.DataFrame()\n",
    "sub_df[\"id\"] = id.cpu().numpy()\n",
    "sub_df[\"pred\"] = y.cpu().numpy()\n",
    "sub_df.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
